---
title: 从线性模型到神经网络
date: 2016-08-23 11:09:48
tags:
- 神经网络
categories:
- 机器学习
---

> 2016年8月29日组会报告

![](/img/从线性模型到神经网络/rank.JPG)

<!--more-->

截止于2016年8月23日, 阿法狗占据世界职业围棋排名榜[Go Ratings](http://www.goratings.org/)的第二名, 再一次证明了人工智能的魅力, 当今的人工智能严格意义上讲, 并没有确切的理论让我们计算出某个节点的具体参数设置, 也就是说在前几十年就出现的基础理论框架之下, 当今的人工智能不得不说还处于实验科学的阶段, 但是我们不可否认的一点是, 数据量的增大和计算能力的提高确实大大提升了模型的效果, 以及各式各样的调参技巧, 这些都是基于大量的试错的, 但是了解并掌握模型的基础理论依然是相当重要的, 下面讲一讲神经网络的基本理论, 在理解神经网络之前, 首先讲解线性模型, 然后从线性模型过渡到神经网络



### **线性模型基本形式**

假设模型由$d$个特征描述, 则可以得出模型的输入向量为$$\vec{x}=(x_1;x_2;\ldots;x_d)$$
- 其中, $x_i$是$\vec{x}$在第$i$个属性上的取值

线性模型的本质是 : 经过模型训练确定一个各个特征的`线性组合`来进行预测的函数, 其形式如下$$f(\vec{x})=w_1x_1+w_2x_2+\ldots+w_dx_d+b$$
一般用向量形式表示: $$f(\vec{x})=\vec{w}^T\vec{x}+b$$
- 其中, 参数向量$\vec{w}=(w_1;w_2;\ldots;w_d)$

> 模型训练的目的就是要确定参数向量 $\vec{w}$和偏置参数$b$, 得到一个确定的线性函数, 当新数据进入模型时, 就会得到相应的输出, 实现预测的目的

### **分类与回归** 

- 分类问题是对`离散变量`的预测, 比如预测明天是阴、晴还是雨，就是一个分类问题

- 回归问题是对`连续变量`的预测, 比如预测明天的气温是多少度，这是一个回归问题

通常我们在线性模型中常说的`线性回归`和`逻辑回归`等模型所确定的线性函数, 有时候我们拿这个训练出来的线性函数做分类任务,有时候做回归任务, 因为二者本质上都是确定一个线性函数$f(\vec{x})$, 只是输出的映射不一样而已

### **线性回归**

给定数据集$$D= \lbrace (\vec{x_1},y_1),(\vec{x_2},y_2),\ldots,(\vec{x_m},y_m) \rbrace$$
其中,

- $\vec{x_i} = (x_1; x_2; \ldots ;x_d)_i$

$x_d$表示第$i$个数据样本中第$d$个特征的取值

- $y_i\in\Re$ 

$y$属于实数集

**线性回归模型试图通过模型训练达到**$$f(\vec{x_i})= \vec{w}^T \vec{x_i}+b, 使得f(\vec{x_i}) \simeq y_i$$

> 那么如何确定$\vec{w}$和$b$呢? 
- 基本思路就是衡量$f(\vec{x_i})$和$y_i$之间的差别, 并找到一种方式去优化参数向量$\vec{w}$和偏置量$b$, 使得$f(\vec{x_i})$和$y_i$之间的差别越来越小, 直到得出最优模型, 下面我们介绍`最小二乘法`和`梯度下降法`

### **最小二乘法**

- 均方误差定义:

$$ J(\vec{w}^T,b) = \sum_{i=1}^{m}(f(\vec{x_i})-y_i)^2 $$

$$ = \sum_{i=1}^{m}(y_i-\vec{w}^T\vec{x_i}-b)^2 $$

- 均方误差具有非常好的几何意义, 本质上就是`欧氏距离`, 基于均方误差最小化进行模型求解的方法称为`最小二乘法`
- $J(\vec{w}^T,b)$也可以称为`损失函数`

在最小二乘法当中, 我们用`均方误差`来衡量模型输出和真实值之间的差异程度, 因此我们只要让均方误差最小化就可以得到最优模型, 即

$$(w^\*,b^\*)= \mathop {\arg min }\limits_{(w,b)} J(\vec{w}^T,b)$$

- 上式的物理意义就是试图找到一个边界, 这个边界可能是一条直线, 一个平面, 或者是一个超平面, 使得所有样本到这个边界的欧氏距离之和最小, 这个求解最优模型的过程就称作`线性回归模型的最小二乘"参数估计"`

#### 优化模型求解过程

对于由$d$个特征描述的$m$个样本组成的数据集$D$, 利用最小二乘法对$\vec{w}$和$b$进行估计, 我们将其描述为线性方程组

$$\vec{\hat{y}}=X\vec{\hat{w}}=
    \begin{pmatrix}
         x_1^1 & x_1^2 & x_1^3 & \cdots & x_1^d & 1 \\\
         x_2^1 & x_2^2 & x_2^3 & \cdots & x_2^d & 1 \\\
         \vdots& \vdots& \vdots& \ddots & \vdots & \vdots \\\ 
         x_m^1 & x_m^2 & x_m^3 & \cdots & x_m^d & 1
    \end{pmatrix}
            \begin{pmatrix}
                 w_1 \\\
                 w_2 \\\
                 \vdots \\\ 
                 w_d \\\
                 b
            \end{pmatrix}
    =
        \begin{pmatrix}
             \vec{x_1}^T & 1 \\\
             \vec{x_2}^T & 1 \\\
             \vdots & \vdots \\\ 
             \vec{x_m}^T & 1
        \end{pmatrix}
        \begin{pmatrix} 
             \vec{w} \\\
             b
        \end{pmatrix}
    =
        \begin{pmatrix}
            \hat{y_1}\\\
            \hat{y_2}\\\
            \vdots   \\\ 
            \hat{y_m}
        \end{pmatrix}
$$

- 其中方程组$\hat{y_m}$表示模型对于第$m$个样本预测的值, $\hat{w}$表示参数向量加上偏置量$b$的增广矩阵

既然我们的线性模型确定了, 那么我们的优化模型也就确定了, 也就是说找到一组确定的$\vec{\hat{w}}$使得均方误差最小, 即:

$$\hat{w}^\*= \mathop {\arg min }\limits_{\hat{w}} J(\hat{w})$$

$$=\mathop {\arg min }\limits_{\hat{w}} (\vec{y}-\vec{\hat{y}})^T(\vec{y}-\vec{\hat{y}})$$

$$=\mathop {\arg min }\limits_{\hat{w}} (\vec{y}-X\vec{\hat{w}})^T(\vec{y}-X\vec{\hat{w}})$$

- 其中, $\vec{y}$ 表示真实的值
- 注意:我们由线性代数的基本性质可知, 一个列向量的转置与其自身相乘就是这个列向量的模的平方

令, $E_\hat{w}=(\vec{y}-X\vec{\hat{w}})^T(\vec{y}-X\vec{\hat{w}})$, 对$\hat{w}$求导得,

$$\frac{\partial E_\hat{w}}{\partial \hat{w}}= 2X^T(X\vec{\hat{w}}-\vec{y}) = 0$$

这里涉及到的矩阵运算不过分讲解,当$X^TX$为满秩矩阵或者正定矩阵时,得到$\vec{\hat{w}}$的最优解, 即:

$$\vec{\hat{w}}^*=(X^TX)^{-1}X^T\vec{y}$$ 

至此, 我们完成了线性回归模型的参数向量的求解

对于一组新的数据$\vec{x_n}$, 加上偏置量$\vec{\hat{x_n}}=(\vec{x_i};1)$, 由我们学习的模型就可以计算其预测值:

$$ \hat{y_n}= f(\vec{\hat{x_n}})= \vec{\hat{x_n}}^T\vec{\hat{w}}^*=\vec{\hat{x_n}}^T(X^TX)^{-1}X^T\vec{y} $$

- 但是在现实生活中$X^TX$大都不是满秩的, 对于有些问题我们选取的特征维数甚至比样本数目还多, 导致$X$的列数多于行数, 显然可以解出多个$\vec{\hat{w}}^*$, 他们都能使模型最优化, 但是不一定每个优化的模型都适合回归任务, 选择哪一个解输出通常由算法的归纳偏好决定, 常见的做法是`引入正则化项`

### **梯度下降法**

























### **逻辑回归**

### **线性可分与线性不可分**

以上我们都是假设我们的数据集是线性可分的, 所谓的线性可分就是

1. 在二维平面上可以找到`一条直线`作为决策边界(两个特征)

    ![](/img/从线性模型到神经网络/separable.JPG)

2. 在三维立体空间中可以找到`一个平面`作为决策边界(三个特征)

    ![](/img/从线性模型到神经网络/separable3d.JPG)

3. 在模型特征多余三个时, 也就是说在高维空间中可以找到一个`线性超平面`作为决策边界

> 是否可以用线性模型解决非线性问题呢? 我们来看一个最简单的例子

1. **与问题**
    ![](/img/从线性模型到神经网络/and.JPG)
    假设由上图的数据可以训练出的模型为$$f(\vec{x})=x_1+x_2-1.5$$, 然后$sigmoid(x_1+x_2-1.5)$得出$y$值
    
2. **或问题**
    ![](/img/从线性模型到神经网络/or.JPG)
    假设由上图的数据可以训练出的模型为$$f(\vec{x})=x_1+x_2-0.5$$, 然后$sigmoid(x_1+x_2-0.5)$得出$y$值
    
3. **异或问题**
    ![](/img/从线性模型到神经网络/xor.JPG)
    
> **用线性模型解决非线性问题**
    
![](/img/从线性模型到神经网络/xor1.JPG)

    

### **神经网络**

### **深度学习**


