---
title: Network In Network
toc: true
tags:
  - NIN
categories:
  - 深度学习
date: 2016-10-05 22:55:04
---
CNN高层特征其实是低层特征通过某种运算的组合, 至于这个运算的目的就是提取高维特征, 线性卷积层采用的是离散卷积运算, 那么能不能改进这个运算使得特征提取更加高效呢, 基于这种思想, 文章提出在每个局部感受野中进行更加复杂的运算，提出了对卷积层的改进算法：`MLP卷积层`

<!--more-->

传统的卷积神经网络一般来说是由：线性卷积层、池化层、全连接层堆叠起来的网络，卷积层通过线性滤波器进行线性卷积运算，然后在接个非线性激活函数，最终生成特征图。一般来说线性卷积层用来提取`线性可分`的特征，但所提取的特征高度非线性时，需要更加多的filters来提取各种潜在的特征，这样就导致filters太多，使得网络参数太多，网络过于复杂对于计算压力太大且容易过拟合。这就产生了传统卷积网络的两个痛点:

1. 线性卷积层在处理高度非线性的底层特征时, 使得网络参数过多

2. 传统的CNN最后一层都是全连接层，参数个数非常之多，容易引起过拟合

我们来看一下文章是怎样解决这两点的:

> 论文: [Network In Network](https://arxiv.org/abs/1312.4400)

### **MLP卷积层**

如下图所示:

![](/img/Network-In-Network/mlpconv.PNG)

- $(a)$: 左图是一个`线性卷积层`, 该层在局部感受野上的运算可以理解为一个单层的网络

- $(b)$: 右图是一个`MLP卷积层`, 该层可以看成是每个卷积的局部感受野中还包含了一个微型的多层网络, 使得计算比传统CNN更加复杂, 从而提高对非线性特征的提取能力

MLP卷积层可以理解为在一个传统的CNN卷积层的基础上把网络在局部感受野的尺度上做深, 增加单个NIN的特征表示能力

### **全局均值池化**

传统的卷积神经网络卷积运算一般是出现在低层网络。对于分类问题，最后一个卷积层的特征图通过量化然后与全连接层连接，最后在接一个softmax逻辑回归分类层。这种网络结构，使得卷积层和传统的神经网络层连接在一起。我们可以把卷积层看做是特征提取器，然后得到的特征再用传统的神经网络进行分类。传统CNN网络中最后全连接层参数过多很容易导致过拟合，造成网络的泛化能力差，Alexnet中使用dropout来防止过拟合提高网络的泛化能力。

文章提出采用全局均值池化的方法，替代传统CNN中的全连接层。与传统的全连接层不同，对每个特征图(feature map)的整张图片进行全局均值池化，这样每张特征图都可以得到一个输出。采用均值池化，可以大大减小网络的参数数量，避免模型过拟合，另一方面它有一个特点，每张特征图相当于一个输出特征，然后这个特征就表示了输出类的特征。这样如果我们在做1000个分类任务的时候，我们网络在设计的时候，最后一层的特征图个数就要选择1000

### **网络结构**

![](/img/Network-In-Network/overall.PNG)

在全局结构图中我们可看到:

- 在局部网络中, 在局部感受野进行卷积运算的基础上又加了两层小的全连接层

> [NIN模型结构](http://ethereon.github.io/netscope/#/gist/3b7a2e24b94158dfafa159b288fa75dc)

1. NIN的实现方式

  ![](/img/Network-In-Network/cccp1.PNG)

  - 由Caffe的prototxt文件可知, 在实现NIN的时候, 全连接层的实现方式为:`设置一个滤波器大小为1,步长为1的卷积层`实现全连接层

  - 由输出尺寸公式得, 内部小网络的增加并不改变输出数据体的大小:

  $$\frac{W-F+2P}{S}+1= \frac{W-1+0}{1}+1=W$$
 
2. 全局均值池化

   ![](/img/Network-In-Network/pool.PNG)
   
   - 在Alexnet网络中最后一个卷积层输出的特征图大小是$6\*6$, pooling的`大小选择6，方法选择：AVE`进行全局均值池化

综上, 在AlexNet的基础上, 只需要简简单单的把卷积核的大小变一下，然后最后一层的全连接层直接用`avg pooling`替换一下就可以实现了

> [机器学习进阶笔记之四 | 深入理解GoogLeNet](https://zhuanlan.zhihu.com/p/22817228)
> [Network In Network学习笔记](http://blog.csdn.net/hjimce/article/details/50458190)
